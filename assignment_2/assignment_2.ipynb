{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Erik Alexander Sandvik <br />\n",
    "Course: STK-IN4300 <br />\n",
    "Assignment number: 2 <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import required packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 1.20\n",
      "Test error: 1.92\n"
     ]
    }
   ],
   "source": [
    "# Read data from file\n",
    "\n",
    "df = pd.read_csv(\"qsar_aquatic_toxicity.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "# Label the columns\n",
    "\n",
    "names = ['TPSA', 'SAacc', 'H050', 'MLOGP', 'RDCHI', 'GATS1p', 'nN', 'C040', 'LC50']\n",
    "df.columns = names\n",
    "\n",
    "\n",
    "# Split dataframe by features and response variable\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "\n",
    "# Split the data into a test and training set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "\n",
    "# Ordinary Least Squares\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate the predicted response using the training set and test set as inputs\n",
    "\n",
    "y_predict_train = reg.predict(X_train)\n",
    "y_predict_test = reg.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate the training and test error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "print('Training error: {:.2f}'.format(MSE(y_train, y_predict_train)))\n",
    "print('Test error: {:.2f}'.format(MSE(y_test, y_predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values:\n",
      "\n",
      "const     6.718007e-19\n",
      "TPSA      4.488061e-17\n",
      "SAacc     5.134547e-10\n",
      "H050      4.744807e-01\n",
      "MLOGP     4.216208e-12\n",
      "RDCHI     5.209703e-03\n",
      "GATS1p    6.349095e-04\n",
      "nN        5.909546e-03\n",
      "C040      7.280372e-01\n",
      "Name: P>|t|, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the significance level of each coefficient in the linear model\n",
    "# scikit-learn doesn't have this implemented\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "\n",
    "mod = sm.OLS(y_train, add_constant(X_train))\n",
    "fii = mod.fit()\n",
    "p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "print(\"P-values:\\n\")\n",
    "print(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 1.22\n",
      "Test error: 2.02\n",
      "\n",
      "\n",
      "P-values:\n",
      "\n",
      "const     5.831407e-20\n",
      "TPSA      4.879928e-13\n",
      "SAacc     1.190513e-08\n",
      "H050      1.693085e-01\n",
      "MLOGP     1.701730e-12\n",
      "RDCHI     1.398751e-02\n",
      "GATS1p    4.313739e-04\n",
      "nN        6.238373e-01\n",
      "C040      3.955416e-01\n",
      "Name: P>|t|, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Same thing as above, but with dichotomization of the count variables\n",
    "\n",
    "\n",
    "# Subscript d for dichotomization\n",
    "\n",
    "X_d = X.copy()\n",
    "\n",
    "count_variables = ['H050', 'nN', 'C040']\n",
    "\n",
    "for cnt_var in count_variables:\n",
    "    X_d.loc[X[cnt_var] > 0, cnt_var] = 1\n",
    "    \n",
    "X_train_d, X_test_d, y_train, y_test = train_test_split(X_d, y, test_size=0.33, random_state=1)\n",
    "\n",
    "reg_d = linear_model.LinearRegression()\n",
    "\n",
    "reg_d.fit(X_train_d, y_train)\n",
    "\n",
    "y_predict_train_d = reg_d.predict(X_train_d)\n",
    "y_predict_test_d = reg_d.predict(X_test_d)\n",
    "\n",
    "print('Training error: {:.2f}'.format(MSE(y_train, y_predict_train_d)))\n",
    "print('Test error: {:.2f}'.format(MSE(y_test, y_predict_test_d)))\n",
    "\n",
    "mod = sm.OLS(y_train, add_constant(X_train_d))\n",
    "fii = mod.fit()\n",
    "p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "print('\\n')\n",
    "print('P-values:\\n')\n",
    "print(p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dichotomization the test error increases and the p-values of the coefficients increases by orders of magnitude, with the exception of the variable 'C040' where the p-value of the corresponding coefficient is reduced by a factor of ~2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test error: 1.48, Standard deviation: 0.17\n",
      "Average test error dichotomized: 1.53, Standard deviation: 0.17\n"
     ]
    }
   ],
   "source": [
    "# Repeat 200 times\n",
    "\n",
    "av_test_error    = 0\n",
    "av_test_error_d  = 0\n",
    "\n",
    "av_test_error2   = 0\n",
    "av_test_error2_d = 0\n",
    "\n",
    "n = 200\n",
    "\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=i)\n",
    "    X_train_d, X_test_d, y_train, y_test = train_test_split(X_d, y, test_size=0.33, random_state=i)\n",
    "    \n",
    "    reg.fit(X_train, y_train)\n",
    "    reg_d.fit(X_train_d, y_train)\n",
    "    \n",
    "    y_predict = reg.predict(X_test)\n",
    "    y_predict_d = reg_d.predict(X_test_d)\n",
    "    \n",
    "    av_test_error += MSE(y_test, y_predict)\n",
    "    av_test_error_d += MSE(y_test, y_predict_d)\n",
    "    \n",
    "    av_test_error2 += MSE(y_test, y_predict)**2\n",
    "    av_test_error2_d += MSE(y_test, y_predict_d)**2\n",
    "    \n",
    "av_test_error    /= n\n",
    "av_test_error_d  /= n\n",
    "av_test_error2   /= n\n",
    "av_test_error2_d /= n\n",
    "\n",
    "std = np.sqrt(av_test_error2 - av_test_error**2)\n",
    "std_d = np.sqrt(av_test_error2_d - av_test_error_d**2)\n",
    "\n",
    "\n",
    "print(\"Average test error: {:.2f}, Standard deviation: {:.2f}\".format(av_test_error, std))\n",
    "print(\"Average test error dichotomized: {:.2f}, Standard deviation: {:.2f}\".format(av_test_error_d, std_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average test errors are lower than what was obtained before (1.92 and 2.02), but that was just for one particular way of splitting the data for training and testing, where I suppose I just got very unlucky (see the standard deviation). \n",
    "\n",
    "My best guess for why dichotomization leads to a worse result is that dichotomization makes the linear model biased. But the variance of the prediction is about the same with or without dichotomization. According to the bias-variance decomposition, the expected prediction error is increased with dichotomization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.92\n",
      "Coefficients:  [ 0.03 -0.02  0.05  0.52  0.43 -0.62 -0.15 -0.04]\n"
     ]
    }
   ],
   "source": [
    "# The first training/test split again\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_predict_test = reg.predict(X_test)\n",
    "\n",
    "error = MSE(y_test, y_predict_test)\n",
    "\n",
    "coeffs = reg.coef_\n",
    "\n",
    "print(\"Error: {:.2f}\".format(error))\n",
    "print(\"Coefficients: \", np.around(coeffs, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Variables (Dummies Generated, First Dummies Dropped): []\n",
      "Eliminated : C040\n",
      "Eliminated : H050\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   LC50   R-squared:                       0.550\n",
      "Model:                            OLS   Adj. R-squared:                  0.543\n",
      "Method:                 Least Squares   F-statistic:                     72.95\n",
      "Date:                Mon, 18 Oct 2021   Prob (F-statistic):           4.03e-59\n",
      "Time:                        15:49:15   Log-Likelihood:                -552.21\n",
      "No. Observations:                 365   AIC:                             1118.\n",
      "Df Residuals:                     358   BIC:                             1146.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept      2.7475      0.270     10.191      0.000       2.217       3.278\n",
      "TPSA           0.0275      0.003      8.901      0.000       0.021       0.034\n",
      "SAacc         -0.0144      0.002     -7.495      0.000      -0.018      -0.011\n",
      "MLOGP          0.5003      0.068      7.411      0.000       0.368       0.633\n",
      "RDCHI          0.4330      0.152      2.850      0.005       0.134       0.732\n",
      "GATS1p        -0.6624      0.172     -3.844      0.000      -1.001      -0.324\n",
      "nN            -0.1422      0.053     -2.687      0.008      -0.246      -0.038\n",
      "==============================================================================\n",
      "Omnibus:                       31.890   Durbin-Watson:                   1.768\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               40.429\n",
      "Skew:                           0.674   Prob(JB):                     1.66e-09\n",
      "Kurtosis:                       3.917   Cond. No.                         581.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "AIC: 1118.4177729055477\n",
      "BIC: 1145.717054380625\n",
      "Final Variables: ['intercept', 'TPSA', 'SAacc', 'MLOGP', 'RDCHI', 'GATS1p', 'nN']\n"
     ]
    }
   ],
   "source": [
    "# Automated Stepwise forward and backward selection in python\n",
    "# See https://github.com/talhahascelik/python_stepwiseSelection for details\n",
    "\n",
    "import stepwiseSelection as ss\n",
    "\n",
    "# Backward selection with AIC\n",
    "final_vars, _ = ss.backwardSelection(X_train, y_train, model_type =\"linear\", elimination_criteria = \"aic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please excuse all the junk.\n",
    "\n",
    "We see that the features 'C040' and 'H050' were eliminated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.92\n",
      "Coefficients:  [ 0.03 -0.01  0.5   0.43 -0.66 -0.14]\n"
     ]
    }
   ],
   "source": [
    "X_fs_aic = X.copy()\n",
    "\n",
    "X_fs_aic.drop('C040', axis=1, inplace=True)\n",
    "X_fs_aic.drop('H050', axis=1, inplace=True)\n",
    "\n",
    "X_train_fs_aic, X_test_fs_aic, y_train_fs_aic, y_test_fs_aic = train_test_split(X_fs_aic, y, test_size=0.33, random_state=1)\n",
    "\n",
    "reg.fit(X_train_fs_aic, y_train_fs_aic)\n",
    "\n",
    "y_predict_test = reg.predict(X_test_fs_aic)\n",
    "\n",
    "error = MSE(y_test, y_predict_test)\n",
    "\n",
    "coeffs = reg.coef_\n",
    "\n",
    "print(\"Error: {:.2f}\".format(error))\n",
    "print(\"Coefficients: \", np.around(coeffs, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Variables (Dummies Generated, First Dummies Dropped): []\n",
      "Eliminated : C040\n",
      "Eliminated : H050\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   LC50   R-squared:                       0.550\n",
      "Model:                            OLS   Adj. R-squared:                  0.543\n",
      "Method:                 Least Squares   F-statistic:                     72.95\n",
      "Date:                Mon, 18 Oct 2021   Prob (F-statistic):           4.03e-59\n",
      "Time:                        15:49:15   Log-Likelihood:                -552.21\n",
      "No. Observations:                 365   AIC:                             1118.\n",
      "Df Residuals:                     358   BIC:                             1146.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept      2.7475      0.270     10.191      0.000       2.217       3.278\n",
      "TPSA           0.0275      0.003      8.901      0.000       0.021       0.034\n",
      "SAacc         -0.0144      0.002     -7.495      0.000      -0.018      -0.011\n",
      "MLOGP          0.5003      0.068      7.411      0.000       0.368       0.633\n",
      "RDCHI          0.4330      0.152      2.850      0.005       0.134       0.732\n",
      "GATS1p        -0.6624      0.172     -3.844      0.000      -1.001      -0.324\n",
      "nN            -0.1422      0.053     -2.687      0.008      -0.246      -0.038\n",
      "==============================================================================\n",
      "Omnibus:                       31.890   Durbin-Watson:                   1.768\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               40.429\n",
      "Skew:                           0.674   Prob(JB):                     1.66e-09\n",
      "Kurtosis:                       3.917   Cond. No.                         581.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "AIC: 1118.4177729055477\n",
      "BIC: 1145.717054380625\n",
      "Final Variables: ['intercept', 'TPSA', 'SAacc', 'MLOGP', 'RDCHI', 'GATS1p', 'nN']\n"
     ]
    }
   ],
   "source": [
    "# Backward selection with BIC\n",
    "final_vars, _ = ss.backwardSelection(X_train, y_train, model_type =\"linear\", elimination_criteria = \"bic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same couple of variables were eliminated, so the error and coefficients are the same with backward selection for both AIC and BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Variables (Dummies Generated, First Dummies Dropped): []\n",
      "Entered : MLOGP \tAIC : 1237.49891950763\n",
      "Entered : TPSA \tAIC : 1179.6284568355122\n",
      "Entered : SAacc \tAIC : 1134.7105633054348\n",
      "Entered : GATS1p \tAIC : 1128.0436044458704\n",
      "Entered : RDCHI \tAIC : 1123.7030733885122\n",
      "Entered : nN \tAIC : 1118.4177729055477\n",
      "Break : Significance Level\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   LC50   R-squared:                       0.550\n",
      "Model:                            OLS   Adj. R-squared:                  0.543\n",
      "Method:                 Least Squares   F-statistic:                     72.95\n",
      "Date:                Mon, 18 Oct 2021   Prob (F-statistic):           4.03e-59\n",
      "Time:                        15:54:51   Log-Likelihood:                -552.21\n",
      "No. Observations:                 365   AIC:                             1118.\n",
      "Df Residuals:                     358   BIC:                             1146.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept      2.7475      0.270     10.191      0.000       2.217       3.278\n",
      "MLOGP          0.5003      0.068      7.411      0.000       0.368       0.633\n",
      "TPSA           0.0275      0.003      8.901      0.000       0.021       0.034\n",
      "SAacc         -0.0144      0.002     -7.495      0.000      -0.018      -0.011\n",
      "GATS1p        -0.6624      0.172     -3.844      0.000      -1.001      -0.324\n",
      "RDCHI          0.4330      0.152      2.850      0.005       0.134       0.732\n",
      "nN            -0.1422      0.053     -2.687      0.008      -0.246      -0.038\n",
      "==============================================================================\n",
      "Omnibus:                       31.890   Durbin-Watson:                   1.768\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               40.429\n",
      "Skew:                           0.674   Prob(JB):                     1.66e-09\n",
      "Kurtosis:                       3.917   Cond. No.                         581.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "AIC: 1118.4177729055477\n",
      "BIC: 1145.717054380625\n",
      "Final Variables: ['intercept', 'MLOGP', 'TPSA', 'SAacc', 'GATS1p', 'RDCHI', 'nN']\n"
     ]
    }
   ],
   "source": [
    "# Forward selection with AIC\n",
    "final_vars, _ = ss.forwardSelection(X_train, y_train, model_type =\"linear\", elimination_criteria = \"aic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H050 and C040 were eliminated yet again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Variables (Dummies Generated, First Dummies Dropped): []\n",
      "Entered : MLOGP \tBIC : 1245.298714214795\n",
      "Entered : TPSA \tBIC : 1191.3281488962598\n",
      "Entered : SAacc \tBIC : 1150.3101527197648\n",
      "Entered : GATS1p \tBIC : 1147.5430912137829\n",
      "Entered : RDCHI \tBIC : 1147.102457510007\n",
      "Entered : nN \tBIC : 1145.717054380625\n",
      "Break : Significance Level\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   LC50   R-squared:                       0.550\n",
      "Model:                            OLS   Adj. R-squared:                  0.543\n",
      "Method:                 Least Squares   F-statistic:                     72.95\n",
      "Date:                Mon, 18 Oct 2021   Prob (F-statistic):           4.03e-59\n",
      "Time:                        15:55:42   Log-Likelihood:                -552.21\n",
      "No. Observations:                 365   AIC:                             1118.\n",
      "Df Residuals:                     358   BIC:                             1146.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept      2.7475      0.270     10.191      0.000       2.217       3.278\n",
      "MLOGP          0.5003      0.068      7.411      0.000       0.368       0.633\n",
      "TPSA           0.0275      0.003      8.901      0.000       0.021       0.034\n",
      "SAacc         -0.0144      0.002     -7.495      0.000      -0.018      -0.011\n",
      "GATS1p        -0.6624      0.172     -3.844      0.000      -1.001      -0.324\n",
      "RDCHI          0.4330      0.152      2.850      0.005       0.134       0.732\n",
      "nN            -0.1422      0.053     -2.687      0.008      -0.246      -0.038\n",
      "==============================================================================\n",
      "Omnibus:                       31.890   Durbin-Watson:                   1.768\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               40.429\n",
      "Skew:                           0.674   Prob(JB):                     1.66e-09\n",
      "Kurtosis:                       3.917   Cond. No.                         581.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "AIC: 1118.4177729055477\n",
      "BIC: 1145.717054380625\n",
      "Final Variables: ['intercept', 'MLOGP', 'TPSA', 'SAacc', 'GATS1p', 'RDCHI', 'nN']\n"
     ]
    }
   ],
   "source": [
    "# Forward selection with BIC\n",
    "final_vars, _ = ss.forwardSelection(X_train, y_train, model_type =\"linear\", elimination_criteria = \"bic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both backward and forward selection, with AIC or with BIC, leads to the exclusion of the variables 'H050' and 'C040', which are the third and the last feature respectively. With or without exclusion, the prediction error is about the same. <br />\n",
    "\n",
    "For your convenience (so you don't have to scroll through all the junk), the coefficients in the linear model without exclusion are <br />\n",
    "\n",
    "[ 0.03 -0.02  0.05  0.52  0.43 -0.62 -0.15 -0.04] <br />\n",
    "\n",
    "With exclusion, the coefficients in the linear model are <br />\n",
    "\n",
    "[ 0.03 -0.01  0.5   0.43 -0.66 -0.14] <br />\n",
    "\n",
    "The coefficients of the features that are kept have nearly unchanged values, only differing at most in the second decimal place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
